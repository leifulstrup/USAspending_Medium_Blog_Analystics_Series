{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyjh5vCGYLhA"
   },
   "source": [
    "# This is the 3rd installment of the Federal Open Data Analytics Edge series.\n",
    "\n",
    "## Focus on Download of a GFY and process that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to previous Medium Article in the series - https://medium.com/@lulstrup/gaining-an-analytics-edge-using-federal-spending-open-data-b91b517f2c04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BD0Y41vtYcQu"
   },
   "outputs": [],
   "source": [
    "import os, glob, pathlib\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO refactor code to use Python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cwd = os.getcwd()\n",
    "download_file_path = os.path.join(original_cwd, \"USAspending_Archive_Downloads\")\n",
    "print(f\"Path to working directory: {original_cwd}\")\n",
    "print(f\"Path to USAspending Archive Zip file Dierectory: {download_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a manual lookup to get the date extension (can be automated with selenium package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_GFY_archive = \"https://www.usaspending.gov/download_center/award_data_archive\"\n",
    "print(f\"Open this link in a new brower tab or window: {url_GFY_archive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You stop at this point and just download files manually. For those who want something more automated, I've created the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CurrentGFY():\n",
    "    if datetime.now().month >= 10: \n",
    "        return datetime.now().year + 1\n",
    "    else:\n",
    "        return datetime.now().year\n",
    "    \n",
    "def Check_Archive_Filename_Format(filename_complete):\n",
    "    base_filename = os.path.basename(filename_complete)\n",
    "    # rule checks\n",
    "    if not base_filename.startswith(\"FY\"):\n",
    "        return False\n",
    "    if not base_filename.endswith(\".zip\") and not base_filename.endswith(\".csv\"):\n",
    "        return False\n",
    "    if \" \" in base_filename:\n",
    "        return False\n",
    "    if len(base_filename.split(\".\")) > 2:\n",
    "        return False\n",
    "    if \"copy\" in base_filename:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def Check_INPUT_FileName_Format_OK(full_download_filename):\n",
    "    if full_download_filename == \"START_PROCESS\":\n",
    "        return False\n",
    "    full_download_filename = full_download_filename.strip() # removes leading and trailing spaces\n",
    "    print(f\"Filename you entered: {full_download_filename}\")\n",
    "    if not full_download_filename.endswith(\".zip\"):\n",
    "        print(\"Error in filename format. Please try again...\")\n",
    "        return False\n",
    "    if not full_download_filename.startswith(\"FY\"):\n",
    "        print(\"Error in filename format. Please try again...\")\n",
    "        return False\n",
    "    print(\"Filename accepted.\")\n",
    "    print()\n",
    "    return True\n",
    "\n",
    "full_download_filename = \"START_PROCESS\"\n",
    "assert Check_INPUT_FileName_Format_OK(full_download_filename) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_download_filename = \"START_PROCESS\"\n",
    "while not Check_INPUT_FileName_Format_OK(full_download_filename):\n",
    "    full_download_filename = input(\"Enter the FULL file name for the latest USAspending.gov archive file name (drag, copy, and paste): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CurrentGFY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAspending Annual (GFY) Archive File Helper Functions\n",
    "\n",
    "def Collect_GFY(filelist_complete_paths):\n",
    "    list_of_GFY = []\n",
    "    for filename_complete in existing_archive_filenames:\n",
    "        filename = filename_complete.split(\"/\")[-1]\n",
    "        filename_GFY = filename[:6]\n",
    "        list_of_GFY.append(filename_GFY)\n",
    "    return dict(Counter(list_of_GFY))\n",
    "\n",
    "#Collect_GFY(existing_archive_filenames)\n",
    "\n",
    "def Get_Filename_Only(filename_to_check_complete_path):\n",
    "    if os.path.isdir(filename_to_check_complete_path):\n",
    "        #print(f\"{filename_to_check_complete_path} is a directory. Ignore\")\n",
    "        return\n",
    "    # assert TBD\n",
    "    return os.path.basename(filename_to_check_complete_path)\n",
    "\n",
    "def Get_GFY_from_file_path(filename_to_check_complete_path):\n",
    "    if os.path.isdir(filename_to_check_complete_path):\n",
    "        #print(f\"{filename_to_check_complete_path} is a directory. Ignore\")\n",
    "        return\n",
    "    filename_GFY = os.path.basename(filename_to_check_complete_path)[:6]\n",
    "    assert filename_GFY[:2] == 'FY' # check this\n",
    "    return filename_GFY\n",
    "\n",
    "def Get_ArchiveDate_from_file_path(filename_to_check_complete_path):\n",
    "    if os.path.isdir(filename_to_check_complete_path):\n",
    "        #print(f\"{filename_to_check_complete_path} is a directory. Ignore\")\n",
    "        return\n",
    "    basefilename = os.path.basename(filename_to_check_complete_path)\n",
    "    if filename_to_check_complete_path.endswith(\".zip\"):\n",
    "        filename_latest_update = basefilename.split(\"_\")[4][0:8]\n",
    "    elif filename_to_check_complete_path.endswith(\".csv\"):\n",
    "        filename_latest_update = basefilename.split(\"_\")[4][0:8]\n",
    "    else:\n",
    "        filename_latest_update =\"Filename_Format_Issue\"\n",
    "    # assert TBD\n",
    "    return filename_latest_update\n",
    "\n",
    "    \n",
    "def Get_Info_On_Archive_Files(download_file_path):\n",
    "    result = []\n",
    "    existing_archive_filenames = sorted(glob.glob(os.path.join(download_file_path, \"*\"))) # display existing files in the folder\n",
    "    for filename_complete in existing_archive_filenames:\n",
    "        if Get_GFY_from_file_path(filename_complete): #ignore None when it encounters a directory\n",
    "            record = {'GFY' : Get_GFY_from_file_path(filename_complete), \n",
    "                      'Archive_Date' : Get_ArchiveDate_from_file_path(filename_complete),\n",
    "                      'Filename' : Get_Filename_Only(filename_complete),\n",
    "                     }\n",
    "            result.append(record.copy())\n",
    "    df_archive_local_file_info = pd.DataFrame(result).sort_values(by='GFY')\n",
    "    df_archive_local_file_info = df_archive_local_file_info[['GFY', 'Archive_Date', 'Filename']]\n",
    "    return df_archive_local_file_info\n",
    "\n",
    "# check if download directory exists, if not, create it\n",
    "# if the directory exists, whoch files should be updated?\n",
    "print(\"WARNING: USAspending Archive files are about 10GB per GFY when unzipped (~1GB zipped).\")\n",
    "print()\n",
    "if not os.path.exists(\"USAspending_Archive_Downloads\"):\n",
    "    os.mkdir(\"USAspending_Archive_Downloads\")\n",
    "\n",
    "assert os.path.exists(\"USAspending_Archive_Downloads\")\n",
    "\n",
    "download_file_path = os.path.join(original_cwd, \"USAspending_Archive_Downloads\")\n",
    "\n",
    "df_archive_local_file_info = Get_Info_On_Archive_Files(download_file_path)\n",
    "df_archive_local_file_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file, is there a newer version in the directory? if so, move the older version to old_files_directory\n",
    "os.chdir(original_cwd)\n",
    "os.chdir(download_file_path) #!!! changing the working Directory\n",
    "\n",
    "if not os.path.exists(\"OLD_FILES_Delete_If_Not_Needed\"):\n",
    "    os.mkdir(\"OLD_FILES_Delete_If_Not_Needed\")\n",
    "\n",
    "assert os.path.exists(\"OLD_FILES_Delete_If_Not_Needed\")\n",
    "\n",
    "if not os.path.exists(\"Expanded_CSV_Files\"):\n",
    "    os.mkdir(\"Expanded_CSV_Files\")\n",
    "\n",
    "assert os.path.exists(\"Expanded_CSV_Files\")\n",
    "\n",
    "existing_archive_filenames_zip = sorted(glob.glob(os.path.join(download_file_path, \"*.zip\")))\n",
    "\n",
    "existing_archive_filenames_csv = sorted(glob.glob(os.path.join(download_file_path, \"Expanded_CSV_Files\", \"*.csv\")))\n",
    "\n",
    "# test whether there are other files in folder that are not zip nor csv\n",
    "existing_archive_filenames_other = sorted(set(glob.glob(os.path.join(download_file_path, \"*\"))) - set(existing_archive_filenames_zip) - set(existing_archive_filenames_csv))\n",
    "# the only other file should be the directory for the old files for deletion\n",
    "if existing_archive_filenames_other != [] and len(existing_archive_filenames_other) == 1:\n",
    "    assert existing_archive_filenames_other[0].split(\"/\")[-1] #confirm this exists - is True\n",
    "else:\n",
    "    for filename in existing_archive_filenames_other:\n",
    "        if not os.path.isdir(filename):\n",
    "            print(f\"unexpected file: {filename} in the download directory. May be ok. Please confirm.\")\n",
    "            print()\n",
    "\n",
    "\n",
    "def Get_Most_Recent_Archive_Name(gfy_filenames):\n",
    "    most_recent_file = gfy_filenames[0]\n",
    "    last_filename_to_check_ArchiveDate = Get_ArchiveDate_from_file_path(most_recent_file)\n",
    "    files_to_move = []\n",
    "    for n, filename in enumerate(gfy_filenames[1:]):\n",
    "        #print(f\"{n+1} : {filename}\")\n",
    "        #print(f\">> {n+1}, most recent: {Get_Filename_Only(most_recent_file)}, filename: {Get_Filename_Only(filename)}\")\n",
    "        filename_ArchiveDate = Get_ArchiveDate_from_file_path(filename)\n",
    "        #print(f\"{filename_ArchiveDate} > {last_filename_to_check_ArchiveDate}\")\n",
    "        if int(filename_ArchiveDate) > int(last_filename_to_check_ArchiveDate): #True if this is a later file\n",
    "            last_filename_to_check_ArchiveDate = filename_ArchiveDate\n",
    "            files_to_move.append(most_recent_file)\n",
    "            most_recent_file = filename\n",
    "            #print(f\"GFY: {filename_GFY} Files with the same GFY: {filename_to_check_GFY} -> {Get_Filename_Only(filename)} archiveDate: {filename_GFY}\")\n",
    "\n",
    "    file_analysis = {\"keep_most_recent_version\" :  most_recent_file, \"move_older_versions_list\" : files_to_move}\n",
    "    \n",
    "    return file_analysis\n",
    "\n",
    "def Identify_Files_to_Move(zip_GFY_dict,existing_archive_filenames_ext):\n",
    "    results = []\n",
    "    for gfy, count in zip_GFY_dict.items():\n",
    "        if count > 1:\n",
    "            gfy_filenames = sorted([filename for filename in existing_archive_filenames_ext if gfy in filename])\n",
    "            move_list = Get_Most_Recent_Archive_Name(gfy_filenames)['move_older_versions_list']\n",
    "            \n",
    "            # look for improperly formated file names in the .zip and .csv files\n",
    "            for filename in gfy_filenames:\n",
    "                if not Check_Archive_Filename_Format(filename):\n",
    "                    move_list.append(filename)\n",
    "            \n",
    "            results.extend(move_list)\n",
    "    return results\n",
    "\n",
    "def Move_Files(move_older_versions_list, extension = \"*.zip\", move_to_folder = \"OLD_FILES_Delete_If_Not_Needed\"):\n",
    "    move_to_list = []\n",
    "    for source_filename in move_older_versions_list:\n",
    "        source_filename_only = os.path.basename(source_filename)\n",
    "        source_dir = os.path.dirname(source_filename)\n",
    "        common_dir = os.path.dirname(source_dir)\n",
    "        destination_dir = os.path.join(source_dir, move_to_folder) #not moving to two peer directories but from higher dir to lower\n",
    "        if not os.path.isdir(destination_dir):\n",
    "            os.mkdir(destination_dir)\n",
    "        \n",
    "        move_result = shutil.move(source_filename, destination_dir)\n",
    "        move_to_list.append(move_result)\n",
    "    return move_to_list\n",
    "\n",
    "def Cleanup_Zip_CSV_Directories(download_file_path):\n",
    "    csv_folder = \"Expanded_CSV_Files\"\n",
    "    print(f\"Analyzing and cleaning up in your Download Zip and CSV files files...\")\n",
    "    print()\n",
    "    for extension in ['*.zip', '*.csv']:\n",
    "        if extension == '*.zip':\n",
    "            existing_archive_filenames_ext = sorted(glob.glob(os.path.join(download_file_path, extension)))\n",
    "        if extension == \"*.csv\":\n",
    "            existing_archive_filenames_ext = sorted(glob.glob(os.path.join(download_file_path, csv_folder, extension)))\n",
    "        zip_GFY_dict = dict(Counter([Get_GFY_from_file_path(filename) for filename in existing_archive_filenames_ext]))\n",
    "        #zip_GFY_dict   \n",
    "        files_to_move = Identify_Files_to_Move(zip_GFY_dict,existing_archive_filenames_ext)\n",
    "        moved_files_new_destination = Move_Files(files_to_move , extension, move_to_folder = \"OLD_FILES_Delete_If_Not_Needed\")\n",
    "        for source, destination in zip(files_to_move, moved_files_new_destination):\n",
    "            print(f\"Source: {source}\")  \n",
    "            print(f\"      -> Destination: {destination}\")\n",
    "    print(\"Done.\")\n",
    "    return\n",
    "        \n",
    "Cleanup_Zip_CSV_Directories(download_file_path)\n",
    "\n",
    "os.chdir(original_cwd)#!!! confirm working directory is the same as when processing started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a list of URL's to download latest USAspending GFY Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Key Step - relies on current structure of how USAspending.gov names and locates annual archive files\n",
    "def Construct_Archive_Download_URL_List():\n",
    "    #example https://files.usaspending.gov/award_data_archive/FY2019_All_Contracts_Full_20200807.zip\n",
    "    prefix = \"https://files.usaspending.gov/award_data_archive/FY\"\n",
    "    midfix = \"_All_Contracts_Full_\"\n",
    "    tail = \".zip\"\n",
    "\n",
    "    latest_archive_extension_date = full_download_filename.split(\".\")[0].split(\"_\")[-1]\n",
    "\n",
    "    url_downloads_list = []\n",
    "    for FY in range(2010,CurrentGFY()+1):\n",
    "      url_download_name = prefix+str(FY)+midfix+latest_archive_extension_date+tail\n",
    "      url_downloads_list.append(url_download_name)\n",
    "    return sorted(url_downloads_list)\n",
    "\n",
    "url_downloads_list = Construct_Archive_Download_URL_List()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide which files to download using the rule:\n",
    "#     if the GFY already exists then, only download the file if there is a newer version\n",
    "#     for the current GFY and the previous GFY on the assumption that curent GFY - 2 is relatively stable (valid assumption?)\n",
    "\n",
    "os.chdir(download_file_path) # change to the download archive folder\n",
    "print(f\"Files will be downloaded to: {os.getcwd()}\")\n",
    "print()\n",
    "print(\"...the download process may take several minutes depending on your internet connections...\")\n",
    "\n",
    "existing_archive_filenames = sorted(glob.glob('*.zip'))\n",
    "\n",
    "# remove FY's that have been added already except for most recent year GFY2020\n",
    "\n",
    "existing_GFY_list = sorted([filename.split(\"/\")[-1][:6] for filename in existing_archive_filenames])\n",
    "\n",
    "\n",
    "#check if the last GFY file archive date has changed since the last download, if so get current GFY and previous GFY\n",
    "if Get_ArchiveDate_from_file_path(existing_archive_filenames[-1]) != Get_ArchiveDate_from_file_path(url_downloads_list[-1]):\n",
    "    existing_GFY_list = existing_GFY_list[:-2] # remove current GFY and previous GFY and get them again since that is the current GFY and being updated every 2 weeks\n",
    "\n",
    "for url_download in url_downloads_list:\n",
    "    filename = url_download.split(\"/\")[-1]\n",
    "    GFY = filename[0:6]\n",
    "    if not GFY in existing_GFY_list: \n",
    "        print(f\"Downloading: {url_download}\")\n",
    "        !wget $url_download\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Skipping file: {filename} already exists in directory.\")\n",
    "\n",
    "print('Downloads completed.')\n",
    "print()\n",
    "zip_archive_size = sum([os.path.getsize(filename) for filename in existing_archive_filenames])/1e9\n",
    "print(f\"USAspending GFY zip file archive size: {zip_archive_size } GB\")\n",
    "\n",
    "os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the Download Zip and CSV Directory Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleanup_Zip_CSV_Directories(download_file_path)\n",
    "\n",
    "os.chdir(original_cwd)#!!! confirm working directory is the same as when processing started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Processing of Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2cTQpRIusyd"
   },
   "outputs": [],
   "source": [
    "# unzip the files using the zipfile package\n",
    "# delete the zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_GFYs_to_Unzip = ['FY2018', 'FY2019', 'FY2020']\n",
    "\n",
    "zip_filenames_all = sorted(glob.glob(os.path.join(download_file_path, \"*.zip\")))\n",
    "for GFY in select_GFYs_to_Unzip:\n",
    "    \n",
    "    aZipFileName = [filename for filename in zip_filenames if GFY in filename][0] #get the Zip file of interest to expand\n",
    "\n",
    "    with zipfile.ZipFile(aZipFileName) as myzip:\n",
    "        print(f\"Extracting files for GFY: {GFY[2:]}...\")\n",
    "\n",
    "        # unzip files to \"Expanded_CSV_Files\" Directory\n",
    "        download_file_path = os.path.join(original_cwd, \"USAspending_Archive_Downloads\")\n",
    "        download_file_path_CSV = os.path.join(download_file_path, \"Expanded_CSV_Files\")\n",
    "\n",
    "        os.chdir(download_file_path) # !!!! change to the download archive folder\n",
    "        print(\"Beginning file Unzip Process. This may take a few minutes to process.\")\n",
    "        \n",
    "        myzip.extractall(path=download_file_path_CSV) # extract them to the directory in path (CSVs only)\n",
    "        # this should overwrite a previous version of the same name\n",
    "\n",
    "        os.chdir(original_cwd) #!!! return to the working directory\n",
    "        print()\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute size of exanded USAspending csv files\n",
    "csv_GB = sum([os.path.getsize(filename) for filename in glob.glob(download_file_path_CSV + \"/*.csv\")])/1e9\n",
    "print(f\"Expanded_CSV-Files folder is {csv_GB} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cleanup again to ensure the files remain consistent for the subsequent steps\n",
    "Cleanup_Zip_CSV_Directories(download_file_path)\n",
    "\n",
    "os.chdir(original_cwd)#!!! confirm working directory is the same as when processing started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Try a Quick Test to Make Sure the files downloaded and expanded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file_path_CSV = os.path.join(download_file_path, \"Expanded_CSV_Files\")\n",
    "csv_filenames = sorted(glob.glob(download_file_path_CSV + \"/*.csv\"))\n",
    "print(f\"There are {len(csv_filenames)} *.csv files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first one into a pandas dataframe to confirm that works\n",
    "print(f\"Reading file: {os.path.basename(csv_filenames[0])}\")\n",
    "df_test = pd.read_csv(csv_filenames[0], low_memory=False, nrows=10) #first 10 rows for testing purposes\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['federal_action_obligation'].sum() #Sum of Federal Obligations for the first 10 rows of the first CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to start analyzing the data.\n",
    "\n",
    "Look for the 4th post in the series with details on processing and analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://opensource.org/licenses/MIT\n",
    "\n",
    "MIT Open Source License Copyright 2020 Leif C Ulstrup\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Analytics_Edge_Blog_Series_#3_Download_a_GFY.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
